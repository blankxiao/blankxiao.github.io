---
title: 机器学习-特征选择
date: 2024-05-28
summary: 除了机器学习算法，特征选择对结果也有很大的影响
tags: [特征选择]
category: 机器学习
comments: true
draft: false
sticky: 0
---

为了解决数据的`维数灾难`  
需要进行数据降维或者`特征选择`  
`特征选择`与降维的区别在于`特征选择`是从所有特征中简单地选  
出相关特征, 选择出来的特征就是原来的特征; 降维则对原来的特征进行了映射变换, 降维后的特征均不再是原来的特征。

1. 计算每个特征的信息增益，使用特征选择算法（如信息增益、基尼系数、互信息等）。
2. 按照信息增益值对特征进行降序排序。
3. 选择前k个特征作为最终选取的特征集合。

- 子集搜索
  前向搜索(每次加上最好的特征)  
  后向(从全集开始 每次去掉最差的特征)  
  双向搜索(设置两个集合来 一个前向 一个后向)

- 子集评价
  信息熵定义 $Ent(D)=-\sum\limits_{(i=1)}^{k}p_{k}log_{2}p_{k}$  
  `D`为数据集 $p_{k}$ 是这一数据类别在总数中的比例  
  信息增益 $Gain(A)=Ent(D)-\sum\limits_{v=1}^{V}\frac{|D^{v}|}{{|D|}}Ent(D^v)$  
  其中 `v`为子集的变量 $|\frac{D^{v}}{D}|$ 为权重 $Ent(D^{v})$ 为对应指标的熵  
  信息增益越大 这个指标就越有用
- Relief(Relevant Features)
  这个思想认为 有用的特征应该在类内差异很小 类间差异大  
  有公式$\sigma^{j}=\sum\limits_{i}(diff(x_{i}^{j},x_{i,nm}^{j})^2-diff(x_{i}^{j},x_{i,nh}^{j})^{2})$  
  其中 $x_{i}^{j}$是任意的点 x_nm代表异类的点 x_nh代表同类的点

信息增益计算示例

| 特征1 | 特征2 | 类别 |
| ----- | ----- | ---- |
| 0     | 1     | 是   |
| 1     | 0     | 否   |
| 1     | 1     | 是   |
| 0     | 0     | 否   |
| 1     | 1     | 是   |

P(是) = 3/5，P(否) = 2/5
整个数据集的信息熵为：  
Entropy(D) = - P(是) _log2(P(是)) - P(否)_ log2(P(否))
= - (3/5) _log2(3/5) - (2/5)_ log2(2/5)
≈ 0.971
根据数据

| 特征1 | 是  | 否  |
| ----- | --- | --- |
| 0     | 1/2 | 1/2 |
| 1     | 2/3 | 1/3 |

计算特征1的信息熵：  
Entropy(特征1=0) = - (1/2) _log2(1/2) - (1/2)_ log2(1/2)  
Entropy(特征1=1) = - (2/3) _log2(2/3) - (1/3)_ log2(1/3)  
权重  
Weight(特征1=0) = 2/5  
Weight(特征1=1) = 3/5  
计算信息增益：  
Information Gain(特征1) = Entropy(D) - (Weight(特征1=0) _Entropy(特征1=0) + Weight(特征1=1)_ Entropy(特征1=1))

计算结果为：  
Information Gain(特征1) ≈ 0.971 - (2/5 _1 + 3/5_ 0.918) ≈ 0.020
